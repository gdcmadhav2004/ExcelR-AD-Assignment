{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e05840e-48c4-48b6-8752-7c391abe40f9",
   "metadata": {},
   "source": [
    "# Write a Python program to calculate the sum of all even numbers between 1 and a given positive integer n\n",
    "# DAY -4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2fc25ff-51af-4f6f-a1a2-5dbc67299fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a positive integer:  20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sum of all even numbers between 1 and 20 is: 110\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def sum_of_evens(n):\n",
    "    # Ensure the input is a positive integer\n",
    "    if n < 1:\n",
    "        return \"Please enter a positive integer.\"\n",
    "    \n",
    "    # Calculate the sum of even numbers\n",
    "    total = sum(i for i in range(2, n + 1, 2))\n",
    "    return total\n",
    "\n",
    "# Input from the user\n",
    "try:\n",
    "    n = int(input(\"Enter a positive integer: \"))\n",
    "    result = sum_of_evens(n)\n",
    "    print(f\"The sum of all even numbers between 1 and {n} is: {result}\")\n",
    "except ValueError:\n",
    "    print(\"Invalid input! Please enter a positive integer.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3550b551-1ee0-42fd-8724-5c17cd7824e8",
   "metadata": {},
   "source": [
    " # Day3_Assignment_ML ^0 NLP\n",
    "Write a Python program that takes a student's marks in three subjects as input.\n",
    "If the average is greater than or equal to 90, print \"Grade: A\".\n",
    "If the average is between 80 and 89, print \"Grade: B\".\n",
    "If the average is between 70 and 79, print \"Grade: C\".\n",
    "Otherwise, print \"Grade: Fail\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "571ab7c4-670d-401f-a588-01bfa6718e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter marks for subject 1:  72\n",
      "Enter marks for subject 2:  90\n",
      "Enter marks for subject 3:  80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grade: B\n"
     ]
    }
   ],
   "source": [
    "def calculate_grade(marks):\n",
    "    # Calculate the average\n",
    "    average = sum(marks) / len(marks)\n",
    "    \n",
    "    # Determine the grade\n",
    "    if average >= 90:\n",
    "        return \"Grade: A\"\n",
    "    elif 80 <= average < 90:\n",
    "        return \"Grade: B\"\n",
    "    elif 70 <= average < 80:\n",
    "        return \"Grade: C\"\n",
    "    else:\n",
    "        return \"Grade: Fail\"\n",
    "\n",
    "# Input marks for three subjects\n",
    "try:\n",
    "    marks = []\n",
    "    for i in range(1, 4):\n",
    "        mark = float(input(f\"Enter marks for subject {i}: \"))\n",
    "        if mark < 0 or mark > 100:\n",
    "            raise ValueError(\"Marks should be between 0 and 100.\")\n",
    "        marks.append(mark)\n",
    "    \n",
    "    # Calculate and display the grade\n",
    "    grade = calculate_grade(marks)\n",
    "    print(grade)\n",
    "except ValueError as e:\n",
    "    print(f\"Invalid input: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630b1b95-a463-40b2-bcb8-484281997ef1",
   "metadata": {},
   "source": [
    "# Day2_Assignment_ML&NLP (1)\n",
    "Create a List, tuple and Dictionary with 5 elements in it and how to access few elements based on the index. Try  with different examples \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac865861-56c1-40be-adbd-6886b2f8b673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First element: 10\n",
      "Last element: 50\n",
      "Slice from index 1 to 3: [20, 30, 40]\n",
      "Modified list: [10, 20, 35, 40, 50]\n"
     ]
    }
   ],
   "source": [
    "# Creating a list\n",
    "my_list = [10, 20, 30, 40, 50]\n",
    "\n",
    "# Accessing elements by index\n",
    "print(\"First element:\", my_list[0])  # 10\n",
    "print(\"Last element:\", my_list[-1])  # 50\n",
    "print(\"Slice from index 1 to 3:\", my_list[1:4])  # [20, 30, 40]\n",
    "\n",
    "# Modifying an element\n",
    "my_list[2] = 35\n",
    "print(\"Modified list:\", my_list)  # [10, 20, 35, 40, 50]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d957c08a-065b-478b-823a-3385cdbc84f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First element: 100\n",
      "Last element: 500\n",
      "Slice from index 1 to 3: (200, 300, 400)\n"
     ]
    }
   ],
   "source": [
    "# Creating a tuple\n",
    "my_tuple = (100, 200, 300, 400, 500)\n",
    "\n",
    "# Accessing elements by index\n",
    "print(\"First element:\", my_tuple[0])  # 100\n",
    "print(\"Last element:\", my_tuple[-1])  # 500\n",
    "print(\"Slice from index 1 to 3:\", my_tuple[1:4])  # (200, 300, 400)\n",
    "\n",
    "# Tuples are immutable, so elements cannot be changed\n",
    "# my_tuple[2] = 350  # This will raise a TypeError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b065b254-4e7a-45df-92ee-8a9e6b0d41dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Alice\n",
      "City: New York\n",
      "Updated dictionary: {'name': 'Alice', 'age': 25, 'city': 'New York', 'profession': 'Engineer', 'hobby': 'Painting', 'country': 'USA'}\n",
      "Keys: ['name', 'age', 'city', 'profession', 'hobby', 'country']\n",
      "Values: ['Alice', 25, 'New York', 'Engineer', 'Painting', 'USA']\n"
     ]
    }
   ],
   "source": [
    "# Creating a dictionary\n",
    "my_dict = {\n",
    "    \"name\": \"Alice\",\n",
    "    \"age\": 25,\n",
    "    \"city\": \"New York\",\n",
    "    \"profession\": \"Engineer\",\n",
    "    \"hobby\": \"Painting\"\n",
    "}\n",
    "\n",
    "# Accessing elements by key\n",
    "print(\"Name:\", my_dict[\"name\"])  # Alice\n",
    "print(\"City:\", my_dict[\"city\"])  # New York\n",
    "\n",
    "# Adding a new key-value pair\n",
    "my_dict[\"country\"] = \"USA\"\n",
    "print(\"Updated dictionary:\", my_dict)\n",
    "\n",
    "# Accessing all keys and values\n",
    "print(\"Keys:\", list(my_dict.keys()))\n",
    "print(\"Values:\", list(my_dict.values()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9826d883-e517-49cb-aa9f-60a9f0139c4c",
   "metadata": {},
   "source": [
    "# Day-1 _Coding_Assignment\n",
    "Write a program in Jupyter Notebook to declare variables of different data types (integer, float, string, and boolean). Print each variable and its type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "412d44eb-c958-4129-895e-52731a17346e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integer Variable: 42 | Type: <class 'int'>\n",
      "Float Variable: 3.14 | Type: <class 'float'>\n",
      "String Variable: Hello, World! | Type: <class 'str'>\n",
      "Boolean Variable: True | Type: <class 'bool'>\n"
     ]
    }
   ],
   "source": [
    "# Declaring variables of different data types\n",
    "integer_var = 42          # Integer\n",
    "float_var = 3.14          # Float\n",
    "string_var = \"Hello, World!\"  # String\n",
    "boolean_var = True        # Boolean\n",
    "\n",
    "# Printing each variable and its type\n",
    "print(\"Integer Variable:\", integer_var, \"| Type:\", type(integer_var))\n",
    "print(\"Float Variable:\", float_var, \"| Type:\", type(float_var))\n",
    "print(\"String Variable:\", string_var, \"| Type:\", type(string_var))\n",
    "print(\"Boolean Variable:\", boolean_var, \"| Type:\", type(boolean_var))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a05bec-614e-44cc-9da8-e6c8f11aa161",
   "metadata": {},
   "source": [
    "# day 5 overview "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f1d89ea-57a0-4aef-9894-5ccc32740775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a text:  Enter a text: Hello world! Hello Python world.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word Frequencies:\n",
      "enter: 1\n",
      "a: 1\n",
      "text: 1\n",
      "hello: 2\n",
      "world: 2\n",
      "python: 1\n"
     ]
    }
   ],
   "source": [
    "# Input text\n",
    "text = input(\"Enter a text: \")\n",
    "\n",
    "# Process the text\n",
    "# Convert text to lowercase and split into words\n",
    "words = text.lower().split()\n",
    "\n",
    "# Create a dictionary to store word frequencies\n",
    "word_counts = {}\n",
    "\n",
    "# Count the frequency of each word\n",
    "for word in words:\n",
    "    # Remove punctuation from the word\n",
    "    word = ''.join(char for char in word if char.isalnum())\n",
    "    if word in word_counts:\n",
    "        word_counts[word] += 1\n",
    "    else:\n",
    "        word_counts[word] = 1\n",
    "\n",
    "# Print the words and their frequencies\n",
    "print(\"\\nWord Frequencies:\")\n",
    "for word, count in word_counts.items():\n",
    "    print(f\"{word}: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f257e3e1-54fd-45af-bf9b-82771994257b",
   "metadata": {},
   "source": [
    "# day 7: Spacy_NLTK_Coding\n",
    "Write a Python program to using NLTK and Spacy\n",
    "Convert text to lowercase.\n",
    "Remove stopwords using NLTK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41fdf4f-28a3-4aee-876b-223a79b3c368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download NLTK data files (run this once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Input text\n",
    "text = input(\"Enter a text: \")\n",
    "\n",
    "# Convert text to lowercase\n",
    "text = text.lower()\n",
    "\n",
    "# Tokenize the text into words\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Get the list of English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Remove stopwords\n",
    "filtered_words = [word for word in words if word.isalnum() and word not in stop_words]\n",
    "\n",
    "# Print the results\n",
    "print(\"\\nFiltered Words (without stopwords):\")\n",
    "print(filtered_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debf1ed6-f2df-4f03-a383-42d41c31ee49",
   "metadata": {},
   "source": [
    "# day 9 \n",
    "NLP_Gensim_BeautifulSoup_Coding\n",
    "Write a Python script that:\n",
    "1. Use Genism to preprocess data from a sample text file, follow basic procedures like tokenization, stemming, lemmatization etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d083d9b6-6a18-4ebc-b2aa-8a92a2a13356",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS, remove_stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Download necessary NLTK data files (run this once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Initialize stemmer and lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Tokenize and lowercase\n",
    "    tokens = simple_preprocess(text, deacc=True)  # Removes punctuation and lowercases\n",
    "    \n",
    "    # Remove stopwords\n",
    "    tokens = [remove_stopwords(token) for token in tokens if token not in STOPWORDS]\n",
    "    \n",
    "    # Apply stemming\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    # Apply lemmatization\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token, pos='v') for token in stemmed_tokens]\n",
    "    \n",
    "    return lemmatized_tokens\n",
    "\n",
    "# Read sample text from a file\n",
    "file_path = \"sample_text.txt\"  # Replace with your file path\n",
    "try:\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "except FileNotFoundError:\n",
    "    print(f\"File '{file_path}' not found. Please provide a valid file path.\")\n",
    "    exit()\n",
    "\n",
    "# Preprocess the text\n",
    "processed_text = preprocess_text(text)\n",
    "\n",
    "# Display the results\n",
    "print(\"Original Text:\")\n",
    "print(text)\n",
    "print(\"\\nProcessed Tokens:\")\n",
    "print(processed_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b98d42-a373-4ba0-aae2-ffbf75ec44e8",
   "metadata": {},
   "source": [
    "# day 11\n",
    "Write a Python script that:\n",
    "1. Tokenizes a sample paragraph into words and sentences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35850102-67ac-4021-9962-51cd8d54aaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# Download necessary NLTK data files\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Sample paragraph\n",
    "paragraph = \"\"\"\n",
    "Natural language processing (NLP) is a field of artificial intelligence (AI) that focuses on the interaction \n",
    "between computers and humans using natural language. The ultimate goal of NLP is to enable computers to \n",
    "understand, interpret, and generate human language in a way that is both valuable and meaningful.\n",
    "\"\"\"\n",
    "\n",
    "# Tokenizing the paragraph into sentences\n",
    "sentences = sent_tokenize(paragraph)\n",
    "print(\"Sentences:\")\n",
    "for sentence in sentences:\n",
    "    print(sentence)\n",
    "\n",
    "# Tokenizing the paragraph into words\n",
    "words = word_tokenize(paragraph)\n",
    "print(\"\\nWords:\")\n",
    "print(words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8fa613-7ba0-44a0-8578-a855bc8fa5e9",
   "metadata": {},
   "source": [
    "# day 13\n",
    "Assignment:\n",
    "Write a Python function to clean a given text by removing special characters and converting it to lowercase. Test it with the input: 'Hello, World! Welcome to NLP 101.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418199fe-61be-4d26-9b2f-df710615926c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters using regex (keeping only alphanumeric characters and spaces)\n",
    "    cleaned_text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# Test the function with the input\n",
    "input_text = 'Hello, World! Welcome to NLP 101.'\n",
    "cleaned_text = clean_text(input_text)\n",
    "\n",
    "print(\"Original Text:\", input_text)\n",
    "print(\"Cleaned Text:\", cleaned_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b6b7c0-f782-4521-8558-8d611115714c",
   "metadata": {},
   "source": [
    "# day 15\n",
    "Examples on Regular Expressions - Coding Assignment\n",
    "Assignment:\n",
    "Write a Python function using regular expressions to extract all email addresses from a given string. Test it with the input: 'Contact us at support@example.com and sales@example.org.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dff574-ce9c-4248-a2f5-2353e0290482",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_emails(text):\n",
    "    # Regular expression pattern to match email addresses\n",
    "    email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
    "    \n",
    "    # Use re.findall() to extract all email addresses\n",
    "    emails = re.findall(email_pattern, text)\n",
    "    \n",
    "    return emails\n",
    "\n",
    "# Test the function with the input\n",
    "input_text = 'Contact us at support@example.com and sales@example.org.'\n",
    "emails = extract_emails(input_text)\n",
    "\n",
    "print(\"Extracted Email Addresses:\", emails)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d237ad74-d7dc-4e77-9a6d-422b7cb98077",
   "metadata": {},
   "source": [
    "# day 18\n",
    "Exercises on Web Scraping - Coding Assignment\n",
    "Assignment:\n",
    "Write a Python script to fetch and print the title of a webpage using the 'requests' and 'BeautifulSoup' libraries. Test it with the URL: 'https://example.com'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57b3caf8-fefc-4a85-b1e5-e8833a341ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Webpage Title: Example Domain\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def fetch_page_title(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the content of the page using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Extract the title of the webpage\n",
    "        title = soup.title.string if soup.title else \"No title found\"\n",
    "        \n",
    "        return title\n",
    "    else:\n",
    "        return f\"Failed to retrieve the webpage. Status code: {response.status_code}\"\n",
    "\n",
    "# Test the function with the URL\n",
    "url = 'https://example.com'\n",
    "title = fetch_page_title(url)\n",
    "\n",
    "print(\"Webpage Title:\", title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5a8ebe-955e-4ad4-9feb-53143a9754cd",
   "metadata": {},
   "source": [
    "# day 20 \n",
    "Text Visualizations with WordCloud on Datasets - Coding Assignment\n",
    "Assignment:\n",
    "Write a Python script to generate a WordCloud from the text: 'data science machine learning artificial intelligence'. Save the WordCloud as an image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756a2218-0246-4b91-b21d-ba55a582d76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wordcloud matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c0169a-3fca-4212-b532-a0d1e5fc7969",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_wordcloud(text, output_image_path):\n",
    "    # Create a WordCloud object\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "    \n",
    "    # Display the WordCloud using matplotlib\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')  # Hide the axes\n",
    "    plt.show()\n",
    "    \n",
    "    # Save the WordCloud as an image file\n",
    "    wordcloud.to_file(output_image_path)\n",
    "    print(f\"WordCloud image saved to {output_image_path}\")\n",
    "\n",
    "# Input text\n",
    "text = 'data science machine learning artificial intelligence'\n",
    "\n",
    "# Output image path\n",
    "output_image_path = 'wordcloud_image.png'\n",
    "\n",
    "# Generate and save the WordCloud\n",
    "generate_wordcloud(text, output_image_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c43305-432d-472a-b8f5-7de9a10a39cf",
   "metadata": {},
   "source": [
    "# day 22\n",
    "Part of Speech Tagging using SpaCy - Coding Assignment\n",
    "Assignment:\n",
    "Write a Python script to perform part-of-speech tagging on the sentence: 'NLP is amazing and fun to learn.' using SpaCy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16742b54-e010-49ad-a096-0547071f8129",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the SpaCy language model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Input sentence\n",
    "sentence = 'NLP is amazing and fun to learn.'\n",
    "\n",
    "# Process the sentence using SpaCy\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# Perform Part-of-Speech tagging and print the results\n",
    "print(\"Word | POS Tag\")\n",
    "print(\"-------------------\")\n",
    "for token in doc:\n",
    "    print(f\"{token.text} | {token.pos_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a973199-4a6f-4d4d-b237-96b3d8371b1b",
   "metadata": {},
   "source": [
    "# day 24\n",
    "Coding Exercise: EDA for Text Data\n",
    "Write a Python program to load a text file, tokenize the text using NLTK, and display the 10 most common words. Use the NLTK library for tokenization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08f9c316-5fff-49ea-81ab-fc2c9d730d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\MADHAV\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the path to your text file:  r\"C:\\Users\\MADHAV\\Desktop\\project file\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: [Errno 22] Invalid argument: 'r\"C:\\\\Users\\\\MADHAV\\\\Desktop\\\\project file\"'\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from collections import Counter\n",
    "\n",
    "# Ensure the required NLTK resources are downloaded\n",
    "nltk.download('punkt')\n",
    "\n",
    "def load_text_file(file_path):\n",
    "    \"\"\"Load the content of a text file.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "def tokenize_text(text):\n",
    "    \"\"\"Tokenize the text using NLTK.\"\"\"\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def get_most_common_words(tokens, n=10):\n",
    "    \"\"\"Get the n most common words from the tokens.\"\"\"\n",
    "    frequency_distribution = FreqDist(tokens)\n",
    "    return frequency_distribution.most_common(n)\n",
    "\n",
    "def main():\n",
    "    # Specify the path to your text file\n",
    "    file_path = input(\"Enter the path to your text file: \")\n",
    "\n",
    "    try:\n",
    "        # Load and process the text file\n",
    "        text = load_text_file(file_path)\n",
    "        tokens = tokenize_text(text)\n",
    "\n",
    "        # Get the 10 most common words\n",
    "        common_words = get_most_common_words(tokens)\n",
    "\n",
    "        # Display the results\n",
    "        print(\"\\nThe 10 most common words are:\")\n",
    "        for word, count in common_words:\n",
    "            print(f\"{word}: {count}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: File not found. Please check the file path.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5c66b8-62bc-4600-9771-50ff3dbde1a6",
   "metadata": {},
   "source": [
    "# day 26 \n",
    "Coding Exercise: Text Similarity Techniques\n",
    "Write a Python program to calculate the cosine similarity between two strings using the Scikit-learn library. You can use the 'TfidfVectorizer' class to transform the text into vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c1018f-5a16-4eca-ae12-60ebc67d66f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def calculate_cosine_similarity(text1, text2):\n",
    "    \"\"\"Calculate the cosine similarity between two strings.\"\"\"\n",
    "    # Initialize TfidfVectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Transform the texts into TF-IDF vectors\n",
    "    tfidf_matrix = vectorizer.fit_transform([text1, text2])\n",
    "\n",
    "    # Compute the cosine similarity\n",
    "    similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
    "\n",
    "    return similarity[0][0]\n",
    "\n",
    "def main():\n",
    "    # Input strings\n",
    "    text1 = input(\"Enter the first string: \")\n",
    "    text2 = input(\"Enter the second string: \")\n",
    "\n",
    "    # Calculate and display cosine similarity\n",
    "    similarity = calculate_cosine_similarity(text1, text2)\n",
    "    print(f\"\\nThe cosine similarity between the two strings is: {similarity:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff471da-17a4-4bea-9068-f85336cf74fd",
   "metadata": {},
   "source": [
    "# day 28\n",
    "Coding Exercise: Named Entity Recognition (NER)\n",
    "Write a Python program to perform Named Entity Recognition (NER) on a given text using SpaCy. Print the entities and their types.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242159ed-7b09-4c4f-b83d-637d0a5ee790",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "def perform_ner(text):\n",
    "    \"\"\"Perform Named Entity Recognition (NER) on the given text using SpaCy.\"\"\"\n",
    "    # Load the SpaCy model (English)\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    # Process the text\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Extract and print entities and their types\n",
    "    print(\"\\nNamed Entities and their Types:\")\n",
    "    for ent in doc.ents:\n",
    "        print(f\"{ent.text}: {ent.label_}\")\n",
    "\n",
    "def main():\n",
    "    # Input text\n",
    "    text = input(\"Enter the text for Named Entity Recognition: \")\n",
    "\n",
    "    # Perform NER\n",
    "    perform_ner(text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cd9e8b-e097-45da-992e-13b26e0ea113",
   "metadata": {},
   "source": [
    "# day 30\n",
    "Coding Exercise: Sentiment Analysis\n",
    "Write a Python program to perform sentiment analysis on a given text using the TextBlob library. Display whether the sentiment is positive, negative, or neutral.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b0405d-bbf7-4bb8-8d59-0a38bc6c5b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    \"\"\"Perform sentiment analysis on the given text using TextBlob.\"\"\"\n",
    "    # Create a TextBlob object\n",
    "    blob = TextBlob(text)\n",
    "\n",
    "    # Get the sentiment polarity\n",
    "    polarity = blob.sentiment.polarity\n",
    "\n",
    "    # Determine the sentiment category\n",
    "    if polarity > 0:\n",
    "        sentiment = \"Positive\"\n",
    "    elif polarity < 0:\n",
    "        sentiment = \"Negative\"\n",
    "    else:\n",
    "        sentiment = \"Neutral\"\n",
    "\n",
    "    return sentiment, polarity\n",
    "\n",
    "def main():\n",
    "    # Input text\n",
    "    text = input(\"Enter the text for sentiment analysis: \")\n",
    "\n",
    "    # Analyze sentiment\n",
    "    sentiment, polarity = analyze_sentiment(text)\n",
    "\n",
    "    # Display the result\n",
    "    print(f\"\\nSentiment: {sentiment} (Polarity: {polarity:.2f})\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8a52fe-6d3c-48b5-a3ef-8b5a737225f3",
   "metadata": {},
   "source": [
    "# day 32\n",
    "Coding Exercise: Exercises Covering All Concepts\n",
    "Write a Python program to load a text file, perform tokenization, calculate the term frequency (TF) of each token, and display the top 5 most frequent tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49510ea-e3e2-4526-af1b-573c612620c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Ensure the required NLTK resources are downloaded\n",
    "nltk.download('punkt')\n",
    "\n",
    "def load_text_file(file_path):\n",
    "    \"\"\"Load the content of a text file.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "def tokenize_text(text):\n",
    "    \"\"\"Tokenize the text using NLTK.\"\"\"\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def calculate_term_frequency(tokens):\n",
    "    \"\"\"Calculate the term frequency of each token.\"\"\"\n",
    "    return Counter(tokens)\n",
    "\n",
    "def display_top_tokens(tf, top_n=5):\n",
    "    \"\"\"Display the top N most frequent tokens.\"\"\"\n",
    "    print(f\"\\nTop {top_n} most frequent tokens:\")\n",
    "    for token, count in tf.most_common(top_n):\n",
    "        print(f\"{token}: {count}\")\n",
    "\n",
    "def main():\n",
    "    # Specify the path to your text file\n",
    "    file_path = input(\"Enter the path to your text file: \")\n",
    "\n",
    "    try:\n",
    "        # Load and process the text file\n",
    "        text = load_text_file(file_path)\n",
    "        tokens = tokenize_text(text)\n",
    "\n",
    "        # Calculate term frequency\n",
    "        tf = calculate_term_frequency(tokens)\n",
    "\n",
    "        # Display the top 5 most frequent tokens\n",
    "        display_top_tokens(tf)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: File not found. Please check the file path.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
